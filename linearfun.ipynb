{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCehQ0tD1oB1vkcbWjCI27",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safal415/ANN-perceptron/blob/main/linearfun.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2eXieGcy53-",
        "outputId": "bda05f77-4955-4ff0-f478-d6c34c0c8c0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Weights = [0.2583736  0.23962182 0.2779594  0.35049915 0.22009295], Bias = 0.599, MSE = 36.1395\n",
            "Epoch 2: Weights = [0.46870837 0.43463455 0.5045068  0.6351589  0.3983205 ], Bias = 1.088, MSE = 23.9564\n",
            "Epoch 3: Weights = [0.63995588 0.5933429  0.68921282 0.86625141 0.54249099], Bias = 1.485, MSE = 15.8910\n",
            "Epoch 4: Weights = [0.77939985 0.72250446 0.83986511 1.05376232 0.65895782], Bias = 1.809, MSE = 10.5517\n",
            "Epoch 5: Weights = [0.89296689 0.82761796 0.96280055 1.20581675 0.75289061], Bias = 2.073, MSE = 7.0171\n",
            "Epoch 6: Weights = [0.98547935 0.91315779 1.0631755  1.32902593 0.82849566], Bias = 2.289, MSE = 4.6772\n",
            "Epoch 7: Weights = [1.06086105 0.98276483 1.14518579 1.42876925 0.8891955 ], Bias = 2.464, MSE = 3.1284\n",
            "Epoch 8: Weights = [1.12230469 1.03940178 1.21224565 1.5094238  0.93777489], Bias = 2.607, MSE = 2.1031\n",
            "Epoch 9: Weights = [1.17240809 1.08547947 1.26713335 1.57455106 0.97649964], Bias = 2.724, MSE = 1.4243\n",
            "Epoch 10: Weights = [1.21328497 1.12295971 1.31210963 1.6270489  1.00721331], Bias = 2.820, MSE = 0.9750\n",
            "Epoch 11: Weights = [1.2466552  1.15343891 1.34901419 1.66927522 1.03141588], Bias = 2.898, MSE = 0.6775\n",
            "Epoch 12: Weights = [1.27391817 1.1782162  1.37934408 1.70314851 1.05032776], Bias = 2.962, MSE = 0.4805\n",
            "Epoch 13: Weights = [1.29621246 1.19834876 1.40431758 1.73022977 1.06494188], Bias = 3.015, MSE = 0.3501\n",
            "Epoch 14: Weights = [1.3144645  1.21469694 1.42492607 1.7517891  1.07606607], Bias = 3.058, MSE = 0.2636\n",
            "Epoch 15: Weights = [1.32942803 1.22796091 1.44197638 1.76885987 1.08435756], Bias = 3.094, MSE = 0.2062\n",
            "Epoch 16: Weights = [1.34171631 1.2387105  1.45612512 1.78228286 1.09035104], Bias = 3.123, MSE = 0.1681\n",
            "Epoch 17: Weights = [1.35182831 1.2474095  1.46790668 1.79274215 1.09448147], Bias = 3.147, MSE = 0.1427\n",
            "Epoch 18: Weights = [1.36016997 1.25443538 1.47775601 1.80079428 1.09710268], Bias = 3.167, MSE = 0.1258\n",
            "Epoch 19: Weights = [1.36707155 1.26009541 1.48602711 1.80689203 1.0985025 ], Bias = 3.184, MSE = 0.1144\n",
            "Epoch 20: Weights = [1.37280174 1.2646397  1.49300816 1.81140378 1.09891502], Bias = 3.198, MSE = 0.1067\n",
            "Epoch 21: Weights = [1.37757911 1.26827187 1.49893373 1.81462919 1.09853065], Bias = 3.210, MSE = 0.1015\n",
            "Epoch 22: Weights = [1.38158147 1.27115771 1.50399481 1.81681203 1.09750421], Bias = 3.220, MSE = 0.0978\n",
            "Epoch 23: Weights = [1.38495346 1.27343222 1.5083469  1.81815059 1.09596162], Bias = 3.229, MSE = 0.0953\n",
            "Epoch 24: Weights = [1.38781272 1.27520534 1.51211663 1.81880617 1.09400523], Bias = 3.236, MSE = 0.0934\n",
            "Epoch 25: Weights = [1.39025494 1.27656661 1.51540714 1.81890993 1.09171824], Bias = 3.243, MSE = 0.0920\n",
            "Epoch 26: Weights = [1.39235793 1.277589   1.51830243 1.81856856 1.08916829], Bias = 3.248, MSE = 0.0909\n",
            "Epoch 27: Weights = [1.39418497 1.27833197 1.52087097 1.8178688  1.08641029], Bias = 3.253, MSE = 0.0900\n",
            "Epoch 28: Weights = [1.39578751 1.27884398 1.52316854 1.81688117 1.0834889 ], Bias = 3.258, MSE = 0.0892\n",
            "Epoch 29: Weights = [1.39720739 1.27916457 1.52524065 1.815663   1.08044032], Bias = 3.262, MSE = 0.0885\n",
            "Epoch 30: Weights = [1.39847858 1.27932598 1.52712439 1.81426089 1.07729398], Bias = 3.266, MSE = 0.0879\n",
            "Epoch 31: Weights = [1.39962874 1.27935453 1.52885004 1.81271269 1.07407372], Bias = 3.269, MSE = 0.0873\n",
            "Epoch 32: Weights = [1.40068029 1.27927172 1.53044233 1.81104916 1.07079887], Bias = 3.272, MSE = 0.0868\n",
            "Epoch 33: Weights = [1.40165146 1.27909514 1.53192145 1.80929526 1.0674851 ], Bias = 3.275, MSE = 0.0863\n",
            "Epoch 34: Weights = [1.40255707 1.27883915 1.53330392 1.80747125 1.06414506], Bias = 3.278, MSE = 0.0858\n",
            "Epoch 35: Weights = [1.40340912 1.27851553 1.53460327 1.80559359 1.060789  ], Bias = 3.281, MSE = 0.0853\n",
            "Epoch 36: Weights = [1.40421735 1.27813395 1.53583058 1.80367558 1.05742518], Bias = 3.283, MSE = 0.0848\n",
            "Epoch 37: Weights = [1.40498965 1.27770234 1.53699494 1.80172804 1.05406024], Bias = 3.286, MSE = 0.0843\n",
            "Epoch 38: Weights = [1.40573244 1.27722722 1.53810383 1.79975969 1.05069954], Bias = 3.288, MSE = 0.0839\n",
            "Epoch 39: Weights = [1.40645088 1.27671397 1.53916341 1.79777759 1.04734735], Bias = 3.290, MSE = 0.0834\n",
            "Epoch 40: Weights = [1.40714915 1.27616705 1.54017877 1.79578745 1.0440071 ], Bias = 3.293, MSE = 0.0830\n",
            "Epoch 41: Weights = [1.40783064 1.27559014 1.54115411 1.79379385 1.04068151], Bias = 3.295, MSE = 0.0825\n",
            "Epoch 42: Weights = [1.40849807 1.27498631 1.54209294 1.79180049 1.03737272], Bias = 3.297, MSE = 0.0821\n",
            "Epoch 43: Weights = [1.40915362 1.27435812 1.54299816 1.78981032 1.03408242], Bias = 3.299, MSE = 0.0817\n",
            "Epoch 44: Weights = [1.40979907 1.27370772 1.54387221 1.78782571 1.03081192], Bias = 3.302, MSE = 0.0812\n",
            "Epoch 45: Weights = [1.4104358  1.27303691 1.54471716 1.78584854 1.02756222], Bias = 3.304, MSE = 0.0808\n",
            "Epoch 46: Weights = [1.41106495 1.27234724 1.54553472 1.7838803  1.0243341 ], Bias = 3.306, MSE = 0.0804\n",
            "Epoch 47: Weights = [1.41168741 1.27164    1.54632639 1.78192216 1.0211281 ], Bias = 3.308, MSE = 0.0800\n",
            "Epoch 48: Weights = [1.41230388 1.27091633 1.54709342 1.77997503 1.01794463], Bias = 3.310, MSE = 0.0796\n",
            "Epoch 49: Weights = [1.4129149  1.2701772  1.5478369  1.77803962 1.01478394], Bias = 3.312, MSE = 0.0792\n",
            "Epoch 50: Weights = [1.41352092 1.26942344 1.54855779 1.77611646 1.01164621], Bias = 3.314, MSE = 0.0788\n",
            "Epoch 51: Weights = [1.41412225 1.26865582 1.54925691 1.77420593 1.00853152], Bias = 3.316, MSE = 0.0784\n",
            "Epoch 52: Weights = [1.41471915 1.26787497 1.549935   1.77230834 1.00543986], Bias = 3.319, MSE = 0.0780\n",
            "Epoch 53: Weights = [1.4153118  1.26708151 1.55059272 1.77042386 1.0023712 ], Bias = 3.321, MSE = 0.0776\n",
            "Epoch 54: Weights = [1.41590033 1.26627594 1.55123065 1.76855263 0.99932545], Bias = 3.323, MSE = 0.0773\n",
            "Epoch 55: Weights = [1.41648484 1.26545877 1.55184934 1.76669469 0.99630247], Bias = 3.325, MSE = 0.0769\n",
            "Epoch 56: Weights = [1.41706538 1.26463043 1.55244927 1.76485008 0.99330213], Bias = 3.327, MSE = 0.0765\n",
            "Epoch 57: Weights = [1.41764198 1.26379134 1.55303089 1.76301876 0.99032425], Bias = 3.329, MSE = 0.0762\n",
            "Epoch 58: Weights = [1.41821465 1.26294186 1.55359463 1.76120069 0.98736863], Bias = 3.331, MSE = 0.0758\n",
            "Epoch 59: Weights = [1.41878338 1.26208236 1.55414087 1.7593958  0.98443508], Bias = 3.333, MSE = 0.0755\n",
            "Epoch 60: Weights = [1.41934816 1.26121318 1.55467    1.75760397 0.98152339], Bias = 3.335, MSE = 0.0751\n",
            "Epoch 61: Weights = [1.41990896 1.26033462 1.55518235 1.75582512 0.97863333], Bias = 3.337, MSE = 0.0748\n",
            "Epoch 62: Weights = [1.42046573 1.259447   1.55567827 1.7540591  0.97576468], Bias = 3.339, MSE = 0.0744\n",
            "Epoch 63: Weights = [1.42101843 1.2585506  1.55615808 1.75230581 0.97291721], Bias = 3.341, MSE = 0.0741\n",
            "Epoch 64: Weights = [1.42156702 1.2576457  1.55662207 1.7505651  0.9700907 ], Bias = 3.343, MSE = 0.0737\n",
            "Epoch 65: Weights = [1.42211144 1.25673256 1.55707055 1.74883682 0.96728491], Bias = 3.345, MSE = 0.0734\n",
            "Epoch 66: Weights = [1.42265164 1.25581145 1.55750381 1.74712085 0.96449961], Bias = 3.347, MSE = 0.0731\n",
            "Epoch 67: Weights = [1.42318757 1.2548826  1.55792212 1.74541702 0.96173458], Bias = 3.349, MSE = 0.0728\n",
            "Epoch 68: Weights = [1.42371918 1.25394626 1.55832574 1.74372519 0.95898957], Bias = 3.351, MSE = 0.0724\n",
            "Epoch 69: Weights = [1.4242464  1.25300267 1.55871494 1.74204521 0.95626437], Bias = 3.353, MSE = 0.0721\n",
            "Epoch 70: Weights = [1.4247692  1.25205205 1.55908998 1.74037694 0.95355875], Bias = 3.355, MSE = 0.0718\n",
            "Epoch 71: Weights = [1.4252875  1.25109461 1.5594511  1.73872021 0.95087248], Bias = 3.357, MSE = 0.0715\n",
            "Epoch 72: Weights = [1.42580127 1.25013058 1.55979855 1.7370749  0.94820535], Bias = 3.359, MSE = 0.0712\n",
            "Epoch 73: Weights = [1.42631044 1.24916017 1.56013255 1.73544084 0.94555713], Bias = 3.361, MSE = 0.0709\n",
            "Epoch 74: Weights = [1.42681498 1.24818358 1.56045335 1.7338179  0.9429276 ], Bias = 3.363, MSE = 0.0706\n",
            "Epoch 75: Weights = [1.42731483 1.247201   1.56076118 1.73220593 0.94031657], Bias = 3.365, MSE = 0.0703\n",
            "Epoch 76: Weights = [1.42780995 1.24621264 1.56105624 1.73060479 0.9377238 ], Bias = 3.367, MSE = 0.0700\n",
            "Epoch 77: Weights = [1.4283003  1.24521868 1.56133876 1.72901434 0.9351491 ], Bias = 3.369, MSE = 0.0697\n",
            "Epoch 78: Weights = [1.42878582 1.24421931 1.56160896 1.72743443 0.93259227], Bias = 3.371, MSE = 0.0694\n",
            "Epoch 79: Weights = [1.42926648 1.24321471 1.56186704 1.72586494 0.93005309], Bias = 3.373, MSE = 0.0691\n",
            "Epoch 80: Weights = [1.42974224 1.24220505 1.5621132  1.72430572 0.92753137], Bias = 3.375, MSE = 0.0688\n",
            "Epoch 81: Weights = [1.43021306 1.24119052 1.56234765 1.72275665 0.92502691], Bias = 3.377, MSE = 0.0685\n",
            "Epoch 82: Weights = [1.43067891 1.24017127 1.56257058 1.72121759 0.92253952], Bias = 3.379, MSE = 0.0683\n",
            "Epoch 83: Weights = [1.43113974 1.23914748 1.56278219 1.71968841 0.92006901], Bias = 3.381, MSE = 0.0680\n",
            "Epoch 84: Weights = [1.43159554 1.23811931 1.56298267 1.718169   0.91761518], Bias = 3.383, MSE = 0.0677\n",
            "Epoch 85: Weights = [1.43204626 1.23708692 1.5631722  1.71665921 0.91517786], Bias = 3.385, MSE = 0.0674\n",
            "Epoch 86: Weights = [1.43249188 1.23605046 1.56335097 1.71515893 0.91275686], Bias = 3.387, MSE = 0.0672\n",
            "Epoch 87: Weights = [1.43293237 1.23501008 1.56351915 1.71366804 0.91035201], Bias = 3.389, MSE = 0.0669\n",
            "Epoch 88: Weights = [1.4333677  1.23396593 1.56367692 1.71218642 0.90796311], Bias = 3.391, MSE = 0.0666\n",
            "Epoch 89: Weights = [1.43379785 1.23291816 1.56382446 1.71071395 0.90559   ], Bias = 3.392, MSE = 0.0664\n",
            "Epoch 90: Weights = [1.4342228  1.23186691 1.56396192 1.70925052 0.90323251], Bias = 3.394, MSE = 0.0661\n",
            "Epoch 91: Weights = [1.43464252 1.23081231 1.56408949 1.707796   0.90089047], Bias = 3.396, MSE = 0.0659\n",
            "Epoch 92: Weights = [1.43505699 1.22975451 1.56420732 1.70635029 0.8985637 ], Bias = 3.398, MSE = 0.0656\n",
            "Epoch 93: Weights = [1.4354662  1.22869364 1.56431557 1.70491329 0.89625204], Bias = 3.400, MSE = 0.0654\n",
            "Epoch 94: Weights = [1.43587013 1.22762982 1.5644144  1.70348487 0.89395534], Bias = 3.402, MSE = 0.0651\n",
            "Epoch 95: Weights = [1.43626875 1.22656319 1.56450397 1.70206494 0.89167343], Bias = 3.404, MSE = 0.0649\n",
            "Epoch 96: Weights = [1.43666207 1.22549386 1.56458442 1.70065338 0.88940615], Bias = 3.406, MSE = 0.0646\n",
            "Epoch 97: Weights = [1.43705005 1.22442196 1.56465591 1.6992501  0.88715334], Bias = 3.408, MSE = 0.0644\n",
            "Epoch 98: Weights = [1.4374327  1.22334761 1.56471859 1.69785499 0.88491486], Bias = 3.410, MSE = 0.0641\n",
            "Epoch 99: Weights = [1.43780999 1.22227093 1.56477259 1.69646796 0.88269055], Bias = 3.412, MSE = 0.0639\n",
            "Epoch 100: Weights = [1.43818192 1.22119202 1.56481805 1.69508889 0.88048027], Bias = 3.414, MSE = 0.0636\n",
            "\n",
            "Final Learned Weights: [1.43818192 1.22119202 1.56481805 1.69508889 0.88048027]\n",
            "Final Learned Bias: 3.413543562028324\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Settings\n",
        "n = 5  # number of features\n",
        "samples = 10\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate random data\n",
        "X = np.random.rand(samples, n)\n",
        "true_weights = np.random.uniform(-1, 1, n)\n",
        "true_bias = 5\n",
        "y = X.dot(true_weights) + true_bias  # Linear function with noise-free data\n",
        "\n",
        "# Initialize model weights\n",
        "weights = np.zeros(n)\n",
        "bias_w = 0\n",
        "lr = 0.01\n",
        "epochs = 100\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_error = 0\n",
        "    for xi, target in zip(X, y):\n",
        "        y_pred = np.dot(xi, weights) + bias_w\n",
        "        error = target - y_pred\n",
        "        weights += lr * error * xi\n",
        "        bias_w += lr * error\n",
        "        total_error += error ** 2\n",
        "    print(f\"Epoch {epoch+1}: Weights = {weights}, Bias = {bias_w:.3f}, MSE = {total_error / samples:.4f}\")\n",
        "\n",
        "print(\"\\nFinal Learned Weights:\", weights)\n",
        "print(\"Final Learned Bias:\", bias_w)"
      ]
    }
  ]
}